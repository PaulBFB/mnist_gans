{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "What are GANs? Generative Adversarial Networks consist of two artificial neural networks (sub-models) that compete in zero-sum game: a generator model and discriminator model. \n",
    "GANs are applicable broadly and used to, among other things, create photo-realistic images for visualization, to model motion patterns, to create 3D models of objects from 2D images and to process astronomical images. \n",
    "GANs are also used to naturally design user interaction with chatbots. GANs are also used in particle physics to accelerate time-consuming detector simulations.\n",
    "\n",
    "\n",
    "### To understand the difference and the interplay between the two sub-models, we first consider discriminative models.\n",
    "Discriminator models usually refers to modelling a classification problem, with the purpose to find a discriminant function that maps a given input onto a specific class.\n",
    "A classic example is spam detection: what is the probability that a given e-mail is spam (y) considering all words in that e-mail $(x) ðŸ¡ª p(y|x)$.\n",
    "\n",
    "\n",
    "This constitutes a supervised learning approach, that fundamentally aims to find the boundary between classes.\n",
    "Generative modelling on the other hand falls into the category of unsupervised learning within the machine learning domain, and aims to discover patterns \n",
    "within a given data set to then generate output that mimics the underlying data. \n",
    "\n",
    "Considering the spam detection example again, a generative approach would be as follows: \n",
    "- 1 assume a given e-mail in span; \n",
    "- 2 what is the probability of seeing these words (relevant for spam detection) in a particular e-mail.  Expressed in statistical terms, a generative approach models the joint probability of an observable variable and the target variable.\n",
    "\n",
    "Generally, a GAN architecture combines these two approaches where the generative model first creates (generates) new data from a vector of latent variables to the desired result space. \n",
    "\n",
    "The generator's aim is to learn to generate results based on a given data distribution to ultimately generate output that is indistinguishable from the ground truth. \n",
    "\n",
    "The discriminator, on the other hand is trained to distinguish the results of the generator from the (fake) data from the real data and labels the generatorsâ€™ output accordingly. \n",
    "In this constellation, the discriminator provides feedback to the generator while simultaneously receiving feedback from the ground truth, the underlying data.\n",
    "\n",
    "![architecture_sketch](./img/GAN_architecture_diagram.png)\n",
    "\n",
    "The two models are organized such that they compete in a zero-sum game (a concept rooted in game theory, where gains and losses cancel each other out, resulting in zero), hence the term â€œadversarialâ€. \n",
    "\n",
    "For instance, the discriminator can be a convolutional network for binary classification, say images. \n",
    "The generator, in a sense, can be seen as an inverse convolutional network that takes random data to produce images. \n",
    "\n",
    "Both models aim to optimize their opposing loss function. The result is a natural (Nash) equilibrium, where the generator produces output that is classified as real 50% of times. \n",
    "\n",
    "Through this combination of models, a unsupervised learning approach is transformed to a supervised approach. The following analogy describes this area of tension.\n",
    "\n",
    ">_â€žWe can think of the generator as being like a counterfeiter, trying to make fake money, and the discriminator as being like police, trying to allow legitimate money and catch counterfeit money. To succeed in this game, the counterfeiter must learn to make money that is indistinguishable from genuine money, and the generator network must learn to create samples that are drawn from the same distribution as the training data.â€œ_\n",
    "\n",
    "![architecture_2](./img/architecture_diagram.png)\n",
    "\n",
    "## GAN Variations\n",
    "There exists a myriad GAN variations and evolutions as can be seen in the following table: \n",
    "\n",
    "![architecture_table](./img/architecture_table.png)\n",
    "\n",
    "However, a good starting point for image-synthesis-based is Deep Convolutional GANs (DCGAN), based on Radfort et alâ€™s groundbreaking work, \n",
    "that condenses to five best practice points guideline points when designing an DCGAN:\n",
    "\n",
    "### Architecture guidelines for stable Deep Convolutional GANs\n",
    "\n",
    "- 1 Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). \n",
    "- 2 Use batchnorm in both the generator and the discriminator. \n",
    "- 3 Remove fully connected hidden layers for deeper architectures.\n",
    "- 4 Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "- 5 Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n",
    "\n",
    "The findings in this paper are earned hard, by scientific rigor and extensive testing.\n",
    "\n",
    "## Architecture Generator\n",
    "The authors suggest to replace _â€œâ€¦deterministic spatial pooling functions (such as maxpooling) with strided convolutionsâ€¦â€œ_ in order to allow the convolutional network its own spatial down sampling, \n",
    "as we have implemented in our MNIST-example. \n",
    "\n",
    "Furthermore, it is recommended to use flattened layers that are directly connected to the output layer (instead of fully connected layers) in the discriminator model, \n",
    "as this yield more model stability and leads to faster convergence. \n",
    "The first layer, then, could be seen as fully connected to the output layer Normalizing each input unit to have zero mean and variance (batch normalization) \n",
    "further stabilizes the learning process, since this supports gradient flow and initialization. \n",
    "\n",
    "Note, however, batch normalization should not be applied to the generatorâ€™s output layers and the discriminatorâ€™s input layers, due to arising problems of model fluctuations and instability.\n",
    "Further,  Radfort et al found that using ReLU activation results in faster learning rates in the domain of image classification, when used in the generator model \n",
    "(all layers except the output layer where Tanh function should be used). \n",
    "\n",
    "A leaky rectified activation should be used in the generator network.\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "> For the complete code, see the [python script](./mnist_gan.py)\n",
    "\n",
    "We have implemented a small wrapper class to show a gan working with the well-documented [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) of handwritten digits, our goal here is to create credible handwritten digits.\n",
    "\n",
    "\n",
    "_____________________________________________________\n",
    "> Sources:\n",
    "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
    "\n",
    ">NIPS 2016 Tutorial: Generative Adversarial Networks\n",
    "* https://en.wikipedia.org/wiki/Generative_model\n",
    "* https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394/\n",
    "* https://pathmind.com/wiki/generative-adversarial-network-gan\n",
    "* https://towardsdatascience.com/gan-objective-functions-gans-and-their-variations-ad77340bce3c-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
